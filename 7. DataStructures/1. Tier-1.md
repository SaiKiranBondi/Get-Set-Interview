### **1. Arrays & Strings**

#### **What they are & Core Concepts (Contiguous Memory, Time Complexity)**

An **Array** is the simplest data structure, acting as the foundation for many others. Its defining characteristic is that it stores a collection of items in a **single, contiguous block of memory**. This means all the elements are physically located next to each other in your computer's RAM.

**Block Diagram: Contiguous Memory**

Imagine an array of 4 integers. In memory, it looks like this:

```
Memory Address: 1000   1004   1008   1012   ...
              +------+------+------+------+
Array:        |  17  |  5   |  -9  |  22  |
              +------+------+------+------+
Index:          [0]    [1]    [2]    [3]
```

This contiguous layout is the source of an array's greatest strength and its biggest weakness.

**Time Complexity of Operations:**

- **Access (by index): `O(1)` - Constant Time**
    
    - **Concept:** This is the superpower of arrays. Because all elements are in a predictable, unbroken line, the computer can instantly calculate the memory address of any element.
        
    - **Formula:** `address_of_element[i] = start_address + (i * size_of_one_element)`
        
    - Since this is a simple mathematical calculation, it takes the same amount of time whether you're accessing the first element or the millionth. It's incredibly fast.
        
- **Search (by value): `O(N)` - Linear Time**
    
    - **Concept:** If you need to find _where_ a certain value is (e.g., "find the index of 22"), the array's structure doesn't help. There's no shortcut. You have to start at the beginning and check every single element one by one until you find it or reach the end.
        
    - In the worst case, you have to look at all `N` elements.
        
- **Insertion / Deletion (at the beginning or middle): `O(N)` - Linear Time**
    
    - **Concept:** This is the primary weakness of arrays. Because the memory block must remain contiguous, inserting an element at the beginning requires shifting _every other element_ one position to the right to make space. Similarly, deleting an element requires shifting every subsequent element to the left to close the gap.
        
    - **Block Diagram: Insertion at Index 1**
        
        ```
        Initial:  [ 17 | 5 | -9 | 22 ]
        
        Insert 10 at index 1:
        1. Shift 22 right: [ 17 | 5 | -9 | __ | 22 ]
        2. Shift -9 right: [ 17 | 5 | __ | -9 | 22 ]
        3. Shift 5 right:  [ 17 | __ | 5 | -9 | 22 ]
        4. Place 10:     [ 17 | 10 | 5 | -9 | 22 ]
        ```
        
    - This shifting operation takes time proportional to the number of elements you have to move (`N`).
        
- **Insertion (at the end): `O(1)` Amortized**
    
    - **Concept:** Adding an element to the end is usually very fast. If there is pre-allocated empty space after the last element, it's a true `O(1)` operation. This is explained further in "Dynamic Arrays" below.
        

Strings

Conceptually, a String is simply an Array of Characters. It inherits all the properties and time complexities of an array. string[i] is an O(1) operation, while inserting a character into the middle of a string is an O(N) operation because it requires shifting.

---
#### **Implementation of Static vs. Dynamic Arrays**

The key difference here is how the array handles its size.

**Static Array**

- **Concept:** The size of the array is **fixed** when it is created. You must declare exactly how many elements it will hold, and this cannot be changed later. Languages like C and Java use this model by default.
    
- **Block Diagram:**
    
    ```
    // C++ code: int myArray[5];
    
    Memory:  Allocates exactly 5 integer-sized slots. No more, no less.
             +---+---+---+---+---+
             |   |   |   |   |   |
             +---+---+---+---+---+
    ```
    
- **Pros:**
    
    - Very fast and memory-efficient (no overhead).
        
    - Predictable memory usage.
        
- **Cons:**
    
    - Inflexible. If you need more space, you can't get it.
        
    - Can be wasteful. If you allocate space for 1000 elements but only use 10, you've wasted 990 slots.
        

**Dynamic Array**

- **Concept:** The size of the array can **grow or shrink** automatically as you add or remove elements. This is the model used by Python's `list`, C++'s `std::vector`, and Java's `ArrayList`. It provides flexibility by hiding the complexity of memory management.
    
- **Underlying Mechanism:** A dynamic array is actually a **static array** under the hood! When it runs out of space, it performs a resizing operation:
    
    1. A new, larger static array is allocated (often double the size of the old one).
        
    2. All elements from the old array are copied to the new, larger array.
        
    3. The old array is deleted from memory.
        
- **Block Diagram: Resizing**
    
    ```
    Capacity: 4
    Array:   [ 10 | 20 | 30 | 40 ]
    
    // User calls: array.append(50) -> Array is full!
    
    1. Allocate new array of capacity 8:
       New Array: [ __ | __ | __ | __ | __ | __ | __ | __ ]
    
    2. Copy elements:
       New Array: [ 10 | 20 | 30 | 40 | __ | __ | __ | __ ]
    
    3. Add the new element:
       New Array: [ 10 | 20 | 30 | 40 | 50 | __ | __ | __ ]
    
    4. The old array is discarded. The variable now points to this new array.
    ```
    
- **Amortized O(1) for Appends:**
    
    - Most of the time, adding an element to the end (`append`) is `O(1)`.
        
    - Occasionally, you have to perform a costly `O(N)` resize operation.
        
    - However, because the array doubles in size each time, the expensive resizes happen less and less frequently as the array grows. When you average the cost over many `append` operations, the cost per operation works out to be **amortized constant time, or O(1)**.
---
### **2. Linked Lists**

#### **What they are & Core Concepts (Node-based, Time Complexity)**

A **Linked List** is a linear data structure where elements are not stored in contiguous memory locations. Instead, elements are stored in individual objects called **nodes**. Each node contains two pieces of information:

1. **The data** (the actual value it holds).
    
2. **A pointer** (a reference or link) to the next node in the sequence.
    

The entry point to the list is a pointer called the `head`, which points to the very first node. The last node in the list points to `null` to signify the end.

**Block Diagram: Non-Contiguous Memory**

Unlike an array, the nodes of a linked list can be scattered all over memory. The pointers are what give the list its structure and order.

```
Memory Address: 1000           2500           1700
              +----------+     +----------+     +----------+
Node:         | 17 | 2500|---->| 5 | 1700 |---->| 22 | null |
              +----------+     +----------+     +----------+
              ^
              |
            head
```

This node-based, non-contiguous structure leads to a completely different set of performance trade-offs compared to arrays.

**Time Complexity of Operations:**

- **Access (by index): `O(N)` - Linear Time**
    
    - **Concept:** This is the primary weakness of linked lists. To get to the `i`-th element, you cannot do a simple calculation like in an array. You **must** start at the `head` and follow the `next` pointers `i` times.
        
    - In the worst case (accessing the last element), you have to traverse all `N` nodes.
        
- **Search (by value): `O(N)` - Linear Time**
    
    - **Concept:** Similar to arrays, to find a specific value, you have to traverse the list from the `head`, checking each node's data one by one.
        
- **Insertion / Deletion (at the beginning): `O(1)` - Constant Time**
    
    - **Concept:** This is the superpower of linked lists. To add a new node to the front, you just need to perform a few pointer re-assignments. You don't need to shift any other elements.
        
    - **Pseudocode for inserting at the beginning:**
        
        ```
        newNode.next = head
        head = newNode
        ```
        
    - This takes the same amount of time regardless of how long the list is.
        
- **Insertion / Deletion (at the end or middle): `O(N)` - Linear Time**
    
    - **Concept:** To insert or delete at the end or in the middle, you first have to **traverse** the list to find the correct position. This traversal takes `O(N)` time. Once you are at the correct node, the actual insertion/deletion (the pointer manipulation) is an `O(1)` operation. The overall complexity is dominated by the `O(N)` search.
        

---

#### **Implementation of Singly Linked List**

This is the most basic form of a linked list.

- **Concept:** Each node contains only one pointer, `next`, which points to the subsequent node in the list. Traversal is strictly one-way (forward).
    
- **Node Structure:** `(data, next_pointer)`
    
- **Block Diagram:**
    
    ```
    head --> [ Node 1 ] --> [ Node 2 ] --> [ Node 3 ] --> null
    ```
    

---

#### **Implementation of Doubly Linked List**

This is an enhanced version of a linked list that provides more flexibility.

- **Concept:** Each node contains **two** pointers: a `next` pointer pointing to the subsequent node, and a `prev` (previous) pointer pointing to the preceding node.
    
- **Node Structure:** `(prev_pointer, data, next_pointer)`
    
- **Block Diagram:**
    
    ```
    head --> [ Node 1 ] <--> [ Node 2 ] <--> [ Node 3 ] --> null
      ^          ^              ^              ^
      |          |              |              |
     null <-- [ Node 1 ] <--> [ Node 2 ] <--> [ Node 3 ]
    ```
    
- **Advantages:**
    
    - Can be traversed in both forward and backward directions.
        
    - Deleting a specific node is easier if you have a pointer to it, as you don't need to traverse from the head to find its predecessor; you can just use its `prev` pointer.
        

---

#### **Implementation of Circular Linked List**

This is a variation where the list forms a circle.

- **Concept:** The `next` pointer of the last node (the `tail`) does not point to `null`. Instead, it points back to the `head` of the list, creating a continuous loop.
    
- **Block Diagram (Singly Circular):**
    
    ```
          +--------------------------------------+
          |                                      |
          V                                      |
    head --> [ Node 1 ] --> [ Node 2 ] --> [ Node 3 ]
    ```
    
- **Key Properties:**
    
    - You can start at any node and traverse the entire list.
        
    - There is no "end" in the traditional sense. You need to be careful to detect when you've completed a full circle to avoid infinite loops.
        
    - Can be either singly or doubly linked.
        
- **Use Cases:** Useful for applications that require round-robin behavior, like managing turns in a game, or creating image carousels on a website.
---
Of course. Here is the detailed breakdown for the next topic, **Stacks**.

---
### **3. Stacks**

#### **What is a Stack & Time Complexity of Operations**

A **Stack** is a linear data structure that follows a particular order in which operations are performed. The order is **LIFO (Last-In, First-Out)**. This means the last element added to the stack will be the first one to be removed.

Core Concept & Analogy:

The classic analogy is a stack of plates. You can only add a new plate to the top, and you can only remove a plate from the top. You cannot easily remove a plate from the middle or bottom without first removing all the plates above it.

**Block Diagram: LIFO Principle**

```
      |     |
      |  C  |  <-- pop() removes C
      |-----|
      |  B  |
      |-----|
      |  A  |
      +-----+
      
push(A) -> push(B) -> push(C)
```

In this diagram, `C` was the last element pushed in, so it will be the first element popped out.

**Core Operations & Time Complexity:**

A stack is defined by three primary operations, all of which are expected to be extremely fast.

- **`push(value)`:** Adds an element to the top of the stack.
    
    - **Time Complexity: `O(1)`** - It takes a constant amount of time, regardless of the stack's size.
        
- **`pop()`:** Removes and returns the element from the top of the stack.
    
    - **Time Complexity: `O(1)`** - It takes a constant amount of time.
        
- **`peek()` or `top()`:** Returns the top element of the stack without removing it.
    
    - **Time Complexity: `O(1)`** - It takes a constant amount of time.
        

---

#### **Implementation using Array/List**

This is the most common and straightforward way to implement a stack. A dynamic array (like a Python `list`) is perfectly suited for this.

- **Concept:** The "top" of the stack is simply the end of the array.
    
- **`push(value)`** is implemented by `array.append(value)`.
    
- **`pop()`** is implemented by `array.pop()`.
    
- **`peek()`** is implemented by accessing the last element (`array[-1]`).
    

**Block Diagram:**

```
// Operation: push(10)
Stack (as Array): [ ] -> [ 10 ]

// Operation: push(20)
Stack (as Array): [ 10 ] -> [ 10, 20 ]

// Operation: peek()
Returns 20. Stack remains [ 10, 20 ]

// Operation: pop()
Returns 20. Stack becomes [ 10 ]
```

**Complexity Note:** While these operations are considered `O(1)`, the `push` operation on a dynamic array has an **amortized** `O(1)` time complexity. This means that _most_ of the time it's `O(1)`, but occasionally the array may need to resize (an `O(N)` operation), but this cost is averaged out over many pushes to be effectively constant time.

---

#### **Implementation using Linked List**

A stack can also be implemented efficiently using a singly linked list.

- **Concept:** The "top" of the stack is the `head` of the linked list.
    
- **`push(value)`:** Create a new node and make it the new `head` of the list.
    
- **`pop()`:** Remove the current `head` and make the next node the new `head`.
    
- **`peek()`:** Return the value of the `head` node.
    

**Block Diagram: Push Operation**

```
// Initial State:
top -> [ Node B ] -> [ Node A ] -> null

// Operation: push(C)
1. Create newNodeC.
2. Set newNodeC.next to point to the current top (Node B).
3. Update the top pointer to point to newNodeC.

// Final State:
top -> [ Node C ] -> [ Node B ] -> [ Node A ] -> null
```

**Complexity Note:** Using a linked list provides a **guaranteed `O(1)` worst-case time complexity** for all operations, as no resizing is ever needed.

---

#### **Implementation of Min-Stack**

This is a famous interview question: design a stack that, in addition to `push`, `pop`, and `top`, supports a `getMin()` function that returns the minimum element currently in the stack in `O(1)` time.

- **Concept:** The key is that we cannot just store the values. With every value we push, we must also store information about the minimum value _at that point in time_.
    
- **Implementation:** A common solution is to have the stack store pairs of `(value, current_min)`.
    

**Block Diagram and Trace:**

```
// Stack stores (value, min_so_far)

// Operation: push(5)
Stack: [ (5, 5) ]

// Operation: push(6)
// min(6, current_min=5) is 5.
Stack: [ (5, 5), (6, 5) ]

// Operation: push(3)
// min(3, current_min=5) is 3.
Stack: [ (5, 5), (6, 5), (3, 3) ]

// Operation: push(4)
// min(4, current_min=3) is 3.
Stack: [ (5, 5), (6, 5), (3, 3), (4, 3) ]

// Operation: getMin()
// Just peek at the second element of the top pair. Returns 3.

// Operation: pop()
// Pops (4, 3). Stack is now [ (5, 5), (6, 5), (3, 3) ]

// Operation: getMin()
// Peek at the new top pair. Returns 3.
```

This design ensures that `getMin()` is always an `O(1)` operation.

---

#### **Implementation of a Queue using two Stacks**

This is another classic problem that tests your understanding of the underlying principles of these data structures. The goal is to simulate FIFO (Queue) behavior using two LIFO (Stack) structures.

- **Concept:** We use two stacks, often called `in_stack` and `out_stack`.
    
    - `in_stack`: Used exclusively for `enqueue` operations.
        
    - `out_stack`: Used exclusively for `dequeue` operations.
        
- **`enqueue(value)`:**
    
    - **Pseudocode:** `in_stack.push(value)`
        
    - This is always an `O(1)` operation.
        
- **`dequeue()`:**
    
    - **Pseudocode:**
        
        ```
        if out_stack is not empty:
            return out_stack.pop()
        else:
            while in_stack is not empty:
                out_stack.push(in_stack.pop())
            return out_stack.pop()
        ```
        
    - **Explanation:** If the `out_stack` has elements, the one on top is the oldest element overall, so we pop it. If the `out_stack` is empty, we must "pour" all elements from the `in_stack` into it. This pouring process reverses the order of the elements, placing the oldest element from the `in_stack` at the top of the `out_stack`, ready to be popped.
        

**Complexity Note:** The `dequeue` operation has an **amortized `O(1)`** time complexity. While the pouring operation can take `O(N)` time, each element is only poured from `in` to `out` once during its entire lifetime in the queue. This expensive operation happens infrequently, and its cost is averaged out over many dequeue calls.

---
### **4. Queues**

#### **What is a Queue & Time Complexity of Operations**

A **Queue** is a linear data structure that follows the **FIFO (First-In, First-Out)** principle. This means the first element added to the queue will be the first one to be removed.

Core Concept & Analogy:

The best analogy is a checkout line at a grocery store. The first person to get in line is the first person to be served. New people always join at the back of the line, and the person at the front is the next one to leave.

**Block Diagram: FIFO Principle**

```
      <-- dequeue() removes A (front)
      +-----+-----+-----+
      |  A  |  B  |  C  |
      +-----+-----+-----+
      Front         Rear
                    ^
                    |
                    enqueue(D) adds here (rear)
```

In this diagram, `A` was the first element enqueued, so it will be the first element dequeued. New elements are always added to the `Rear` (or back).

**Core Operations & Time Complexity:**

A queue is defined by three primary operations, which are expected to be `O(1)`.

- **`enqueue(value)`:** Adds an element to the rear (back) of the queue.
    
    - **Time Complexity: `O(1)`**
        
- **`dequeue()`:** Removes and returns the element from the front of the queue.
    
    - **Time Complexity: `O(1)`**
        
- **`front()` or `peek()`:** Returns the front element of the queue without removing it.
    
    - **Time Complexity: `O(1)`**
        

---

#### **Implementation using Linked List**

Using a linked list is a very natural way to implement a queue, as it handles the growing and shrinking size seamlessly.

- **Concept:** To achieve `O(1)` complexity for both `enqueue` and `dequeue`, we need to keep track of both the `head` and the `tail` of the linked list.
    
    - The `head` pointer represents the **front** of the queue.
        
    - The `tail` pointer represents the **rear** of the queue.
        
- **`enqueue(value)`:** A new node is added after the current `tail`, and the `tail` pointer is updated to this new node.
    
- **`dequeue()`:** The `head` node is removed, and the `head` pointer is updated to the next node in the list.
    

**Block Diagram: Enqueue Operation**

```
// Initial State:
head -> [ Node A ] -> [ Node B ] <- tail

// Operation: enqueue(C)
1. Create newNodeC.
2. Set tail.next to point to newNodeC.
3. Update the tail pointer to point to newNodeC.

// Final State:
head -> [ Node A ] -> [ Node B ] -> [ Node C ] <- tail
```

This implementation provides a guaranteed `O(1)` worst-case time complexity for both `enqueue` and `dequeue`.

---

#### **Implementation using Array (Circular Queue)**

Implementing a queue with a standard array is inefficient. If you dequeue from the front (index 0), you would have to shift all other elements to the left, making `dequeue` an `O(N)` operation. To solve this, we use a **Circular Queue**.

- **Concept:** A circular queue uses a fixed-size array and two pointers: `front` and `rear`. It treats the array as if it's circular, meaning the end of the array wraps around to the beginning. When an element is dequeued, we simply move the `front` pointer forward. When an element is enqueued, we move the `rear` pointer forward. The modulo operator (`%`) is used to handle the wrap-around logic.
    
- **`enqueue(value)`:** Add the element at the `rear` index and increment `rear`. `rear = (rear + 1) % capacity`.
    
- **`dequeue()`:** Return the element at the `front` index and increment `front`. `front = (front + 1) % capacity`.
    

**Block Diagram: Circular Behavior**

```
Array (capacity 5): [ __ | C | D | E | B ]
                       ^       ^
                     rear    front
// rear is at index 1, front is at index 4. The queue contains [B, C, D, E].

// Operation: dequeue()
// Returns B. front moves to (4+1)%5 = 0.
Array (capacity 5): [ __ | C | D | E | __ ]
                       ^   ^
                     rear front

// Operation: enqueue(F)
// Adds F at rear. rear moves to (1+1)%5 = 2.
Array (capacity 5): [ F | C | D | E | __ ]
                           ^   ^
                         rear front
```

This design achieves `O(1)` complexity for both operations, but has a fixed capacity.

---

#### **Implementation of Deque (Double-Ended Queue)**

A Deque (pronounced "deck") is a generalization of a queue.

- **What it is:** A linear data structure that allows insertion and deletion of elements from **both ends**—the front and the rear.
    
- **Core Operations:**
    
    - `add_front(value)`
        
    - `add_rear(value)`
        
    - `remove_front()`
        
    - `remove_rear()`
        
- **Implementation:** It is most commonly implemented using a **doubly linked list**, as this provides the flexibility to easily modify both the `head` and `tail` of the list in `O(1)` time.
    

---

#### **Implementation of a Stack using Queues**

This is a classic problem that inverts the LIFO/FIFO principles. The goal is to simulate LIFO (Stack) behavior using one or more FIFO (Queue) structures.

- **Concept (Two-Queue Approach):** Use a primary queue (`q1`) and a helper queue (`q2`).
    
    - **`push(value)` (Costly Push):**
        
        1. Enqueue the new `value` into the empty `q2`.
            
        2. Dequeue all elements from `q1` one by one and enqueue them into `q2`.
            
        3. Swap the names of `q1` and `q2`.
            
        
        - **Result:** The newest element is now at the front of the primary queue, ready to be dequeued (popped).
            
    - **`pop()`:** Simply dequeue from `q1`.
        
- **Complexity:** In this approach, `push` becomes an `O(N)` operation, while `pop` is `O(1)`. (An alternative exists where `push` is `O(1)` and `pop` is `O(N)`).
---
