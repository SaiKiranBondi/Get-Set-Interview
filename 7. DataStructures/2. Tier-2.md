### **5. Hash Tables**

#### **What they are & Core Concepts (Hash Functions, Collisions)**

A **Hash Table** is a data structure that maps **keys** to **values** for highly efficient lookups. It's the underlying structure that powers data types like Python's `dictionary` and Java's `HashMap`.

Core Concept & Analogy:

Imagine a library with a massive, perfectly organized filing system. To find a book (a value), you don't scan every shelf. Instead, you look up the book's title (the key) in a catalog, which tells you the exact shelf and location to go to. A hash table works similarly.

The "magic" of a hash table comes from two main components:

1. **An underlying array:** This array (often called "buckets" or "slots") is where the data is actually stored.
    
2. **A Hash Function:** This is a special function that takes a `key` as input and instantly computes an index into the underlying array. `index = hash_function(key)`.
    

**Block Diagram: The Hashing Process**

```
      +-----------+
Key:  | "apple"   |
      +-----------+
            |
            V
+-----------------------+
|    hash_function()    |  --> Computes an index, e.g., 4
+-----------------------+
            |
            V
        Array of Buckets
  [0] [1] [2] [3] [4] [5] ...
                  +-----------+
                  |  Value    |
                  | for "apple"|
                  +-----------+
```

The Problem: Collisions

A hash function might compute the same index for two different keys. This is called a collision. For example, hash("apple") might be 4, and hash("plum") might also be 4. You can't store both values in the same single slot.

Collision Resolution: Chaining

The most common way to handle collisions is called chaining. Instead of storing the value directly in the array slot, we store a linked list of key-value pairs at that index.

**Block Diagram: Collision with Chaining**

```
Key: "plum" -> hash_function() -> Index 4

        Array of Buckets
  [0] [1] [2] [3] [4] [5] ...
                  |
                  V
                [ "apple": value1 ] -> [ "plum": value2 ] -> null
```

When a collision occurs, we just add the new key-value pair to the end of the linked list at that bucket. When we search, we go to the correct bucket and then traverse the short linked list to find the matching key.

**Time Complexity of Operations:**

- **Insertion, Deletion, Search: `O(1)` Average Case**
    
    - **Concept:** In the average case, a good hash function distributes keys evenly, keeping the linked lists at each bucket very short (or empty). The process is: hash the key to find the bucket (`O(1)`), then access the bucket (`O(1)`). The traversal of the short list is negligible.
        
- **`O(N)` Worst Case:**
    
    - **Concept:** In the worst case, a poorly designed hash function maps _every single key_ to the same bucket. This turns the hash table into one giant linked list. A lookup would then require traversing all `N` elements in that list.
        

---

#### **Key Implementations**

**Hash Map (Dictionary)**

- **What it is:** The classic implementation of a hash table that stores **key-value pairs**.
    
- **Use Case:** Perfect for when you need to associate one piece of data with another, like mapping a user's ID to their profile information.
    

**Hash Set**

- **What it is:** A simplified version that only stores unique **keys**. You can think of it as a hash map where the value is always just `True`.
    
- **Use Case:** Its primary purpose is to provide a highly efficient way to check for the existence of an item in a collection or to store a collection of unique items. `is_item_present = item in my_hash_set` is an `O(1)` average time operation.
    

---

#### **Important Variations: Implementation of LRU Cache**

An **LRU (Least Recently Used) Cache** is a classic and important data structure problem that demonstrates a powerful combination of data structures. It's a cache with a fixed size that automatically evicts the least recently used item when it becomes full.

**The Challenge:** We need to support two operations in `O(1)` time:

- **`get(key)`:** Retrieve a value and mark it as "recently used."
    
- **`put(key, value)`:** Insert or update a value and mark it as "recently used." If the cache is full, evict the least recently used item first.
    

**The Solution: Combining a Hash Map and a Doubly Linked List**

Neither a hash map nor a linked list can solve this alone efficiently. But together, they are perfect.

1. **Hash Map:** Provides the `O(1)` lookup. It will store `key -> node` pairs, where `node` is an actual node in our linked list. This lets us jump directly to any item in the list.
    
2. **Doubly Linked List:** Provides the `O(1)` eviction/reordering. The list will be ordered by recency. We'll move any accessed node to the front of the list. When the cache is full, we evict the node at the back of the list. A doubly linked list is crucial because to remove a node, we need to update the `next` pointer of the node _before_ it and the `prev` pointer of the node _after_ it, which we can do in `O(1)` time.
    

**Block Diagram: LRU Cache Structure**

```
+-------------------------------------------------+
|                   HASH MAP                      |
|  {                                              |
|    "keyA": pointer_to_NodeA,                     |
|    "keyB": pointer_to_NodeB,                     |
|    "keyC": pointer_to_NodeC                      |
|  }                                              |
+-------------------------------------------------+
      |                 |                 |
      |                 |                 |
      V                 V                 V
+-------------------------------------------------+
|             DOUBLY LINKED LIST                  |
|                                                 |
|  head -> [NodeC] <-> [NodeB] <-> [NodeA] <- tail |
| (Most         (Least         |
|  Recently      Recently Used) |
|  Used)                        V                 |
|                               Evict this one    |
+-------------------------------------------------+
```

**Trace of `get("keyB")`:**

1. Use the hash map to find `pointer_to_NodeB` in `O(1)`.
    
2. Use that pointer to access `NodeB` directly.
    
3. "Unlink" `NodeB` from its current position in the list (`O(1)` because it's doubly linked).
    
4. Move `NodeB` to the front of the list (`O(1)`).
    
5. Return the value from `NodeB`.
---
### **6. Trees**

#### **What they are & Core Concepts / Terminology**

A **Tree** is a widely used hierarchical data structure that simulates a tree structure with a set of linked nodes. Unlike linear data structures (Arrays, Linked Lists, Stacks, Queues) which have a logical start and end, trees are non-linear.

Core Concept:

A tree is a collection of nodes connected by directed or undirected edges. It has one special node called the Root, which is the starting point of the tree. Each node can have zero or more child nodes. A node that has no children is called a leaf node.

**Block Diagram & Key Terminology:**

```
              ( A )  <-- Root
              / | \
             /  |  \
            /   |   \
          ( B ) ( C ) ( D )  <-- Children of A. A is the Parent of B, C, D.
          / \     |          B, C, D are Siblings.
         /   \    |
       ( E ) ( F )( G )  <-- E, F, G are Leaf Nodes (no children).
```

- **Node:** An entity that contains a key or value and pointers to its child nodes. (e.g., A, B, C...)
    
- **Edge:** The link between two nodes.
    
- **Root:** The topmost node in a tree. It is the only node that does not have a parent. (A)
    
- **Parent:** A node that has an edge to a child node. (A is the parent of B).
    
- **Child:** A node that has a parent node. (B is a child of A).
    
- **Siblings:** Nodes that share the same parent. (B, C, and D are siblings).
    
- **Leaf Node:** A node with no children. (E, F, G, D).
    
- **Path:** A sequence of nodes and edges from one node to another. (Path from A to F is A -> B -> F).
    
- **Height of a Tree:** The number of edges on the longest path from the root to a leaf.
    
- **Depth of a Node:** The number of edges from the root to that node. (Depth of B is 1).
    

---

#### **Traversal Strategies**

Traversal is the process of visiting (checking or updating) each node in a tree exactly once. The way you traverse a tree is crucial and depends on the problem you're trying to solve. There are two main approaches: Depth-First Search (DFS) and Breadth-First Search (BFS).

Depth-First Search (DFS) Strategies

DFS explores as far as possible down one branch before backtracking. It's typically implemented with recursion.

- **Pre-order Traversal (`Root, Left, Right`)**
    
    1. Visit the current node.
        
    2. Recursively traverse the left subtree.
        
    3. Recursively traverse the right subtree.
        
    
    - **Use Case:** Useful for creating a copy of the tree.
        
    - **Order for diagram above (assuming B->C->D order):** A, B, E, F, C, G, D
        
- **In-order Traversal (`Left, Root, Right`)**
    
    1. Recursively traverse the left subtree.
        
    2. Visit the current node.
        
    3. Recursively traverse the right subtree.
        
    
    - **Use Case:** In a Binary Search Tree (BST), this traversal visits nodes in ascending order.
        
    - **Order for diagram above:** E, B, F, A, G, C, D
        
- **Post-order Traversal (`Left, Right, Root`)**
    
    1. Recursively traverse the left subtree.
        
    2. Recursively traverse the right subtree.
        
    3. Visit the current node.
        
    
    - **Use Case:** Useful for deleting nodes from a tree (you delete children before the parent).
        
    - **Order for diagram above:** E, F, B, G, C, D, A
        

**Breadth-First Search (BFS) Strategy**

- **Level-order Traversal**
    
    1. Visit nodes level by level, from top to bottom and left to right.
        
    2. This is **not** recursive. It is implemented with a **Queue**.
        
    
    - **Pseudocode:**
        
        ```
        queue.enqueue(root)
        while queue is not empty:
            node = queue.dequeue()
            visit(node)
            for each child of node:
                queue.enqueue(child)
        ```
        
    - **Order for diagram above:** A, B, C, D, E, F, G
        

---

#### **Key Implementations**

Implementation of a generic N-ary Tree node

This is a tree where a node can have any number of children.

- **Concept:** The node structure must be ableto hold references to multiple children. A common way is to have a `children` property which is a list or array of pointers to other nodes.
    
- **Pseudocode for Node Structure:**
    
    ```
    class TreeNode:
        value: data
        children: list of TreeNode
    ```
    

Implementation of a Binary Tree node

This is the most common type of tree in interviews, where each node has at most two children.

- **Concept:** The node structure is simplified to have two specific pointers: `left` and `right`.
    
- **Pseudocode for Node Structure:**
    
    ```
    class BinaryTreeNode:
        value: data
        left: BinaryTreeNode  // Pointer to the left child
        right: BinaryTreeNode // Pointer to the right child
    ```
    

This specialized structure is the foundation for Binary Search Trees, Heaps (conceptually), and many other tree-based algorithms.

---
### **7. Binary Search Trees (BST)**

#### **What it is & The Ordering Property**

A **Binary Search Tree (BST)** is a binary tree with a crucial additional property that makes it incredibly efficient for searching. This property governs the placement of every node in the tree:

The BST Ordering Property:

For any given node N in the tree:

1. All values in the **left subtree** of `N` must be **less than** `N`'s value.
    
2. All values in the **right subtree** of `N` must be **greater than** `N`'s value.
    
3. Both the left and right subtrees must also be binary search trees.
    

This property must hold true for every single node in the tree.

**Block Diagram: A Valid BST**

```
              ( 8 )
              /   \
             /     \
           ( 3 )   ( 10 )
           /   \       \
          /     \       \
        ( 1 )   ( 6 )   ( 14 )
                /   \
               /     \
             ( 4 )   ( 7 )
```

Notice how at every node (e.g., node 3), everything to its left (1) is smaller, and everything to its right (6, 4, 7) is larger. This structure allows us to discard half of the remaining tree at every step of a search, which is why it's so fast.

---

#### **Core Operations & Complexity**

The ordering property dictates how all operations are performed.

**1. Search**

- **Concept:** To find a `target` value, you start at the root. At each node, you compare your `target` to the node's value and make one of three decisions:
    
    - If `target == node.value`, you've found it.
        
    - If `target < node.value`, you know it can _only_ be in the left subtree, so you go left.
        
    - If `target > node.value`, you know it can _only_ be in the right subtree, so you go right.
        
- **Pseudocode:**
    
    ```
    function search(node, target):
        if node is null or node.value == target:
            return node
    
        if target < node.value:
            return search(node.left, target)
        else:
            return search(node.right, target)
    ```
    

**2. Insertion**

- **Concept:** Insertion works just like a search. You follow the search path down the tree as if you were looking for the value you want to insert. When you reach a `null` spot, that's where the new node belongs.
    
- **Pseudocode:**
    
    ```
    function insert(node, value):
        if node is null:
            return createNewNode(value)
    
        if value < node.value:
            node.left = insert(node.left, value)
        else if value > node.value:
            node.right = insert(node.right, value)
    
        return node // Return the (possibly modified) node pointer
    ```
    

**3. Deletion**

- **Concept:** Deletion is the most complex operation because you must preserve the BST property after removing a node. There are three cases:
    
    - **Case 1: Node to be deleted is a leaf.** This is easy. You just remove it by setting its parent's pointer to `null`.
        
    - **Case 2: Node has one child.** Also easy. You "bypass" the node by linking its parent directly to its child.
        
    - **Case 3: Node has two children.** This is the tricky case. You cannot simply remove it. Instead, you must replace it with another node from the tree that maintains the BST order. Typically, you replace it with either:
        
        - Its **in-order successor:** The smallest value in its right subtree.
            
        - Its in-order predecessor: The largest value in its left subtree.
            
            You then delete the successor/predecessor from its original location (which will be an easier Case 1 or Case 2 deletion).
            

**Block Diagram: Deletion (Case 3)**

```
// Goal: Delete node 8
              ( 8 )
              /   \
            ( 3 )   ( 10 )
                        \
                        ( 14 )
                        /
                      ( 13 )  <-- In-order successor of 8

1. Find the in-order successor (smallest value in the right subtree), which is 13.
2. Replace 8's value with 13's value.
3. Delete the original node 13 from its spot (which is a simple Case 1 or Case 2 deletion).

Resulting Tree:
              ( 13 )
              /    \
            ( 3 )    ( 10 )
                         \
                         ( 14 )
```

**Time Complexity (for Search, Insertion, and Deletion):**

- **Average Case: `O(log N)`**
    
    - **Concept:** This occurs when the tree is **balanced**. A balanced tree looks bushy, with the left and right subtrees having roughly the same height. In this case, each comparison allows you to discard about half of the remaining nodes, which is the same principle as binary search on an array. The height of the tree is `log N`.
        
- **Worst Case: `O(N)`**
    
    - **Concept:** This occurs when the tree is completely **unbalanced** or **skewed**. This happens if you insert elements in an already sorted order (e.g., 1, 2, 3, 4, 5). The tree degenerates into a straight line—effectively becoming a linked list.
        
    - **Block Diagram: A Skewed Tree**
        
        ```
              ( 1 )
                 \
                 ( 2 )
                    \
                    ( 3 )
                       \
                       ( 4 )
                          \
                          ( 5 )
        ```
        
    - In this scenario, a search for the value `5` would require visiting every single node, resulting in `O(N)` time complexity. This is why self-balancing BSTs (like AVL or Red-Black Trees) exist.
---
### **8. Heaps**

#### **What it is & The Heap Property (Min-Heap vs. Max-Heap)**

A **Heap** is a specialized tree-based data structure that satisfies the "heap property". It is also a **"nearly complete" binary tree**. This "complete" property is crucial because it allows the heap to be stored efficiently in an array.

- **Complete Binary Tree:** A binary tree where every level, except possibly the last, is completely filled, and all nodes in the last level are as far left as possible.
    

The **Heap Property** defines the relationship between a parent node and its children. There are two types of heaps:

**1. Min-Heap**

- **Property:** The value of any given node is **less than or equal to** the values of its children.
    
- **Result:** The smallest element in the entire collection is always at the root of the tree.
    

**Block Diagram: Min-Heap**

```
              ( 10 )
              /    \
            ( 15 )  ( 30 )
            /   \
          ( 40 )( 50 )
```

_Notice how every parent (e.g., 10, 15) is smaller than its children._

**2. Max-Heap**

- **Property:** The value of any given node is **greater than or equal to** the values of its children.
    
- **Result:** The largest element in the entire collection is always at the root of the tree.
    

**Block Diagram: Max-Heap**

```
              ( 50 )
              /    \
            ( 40 )  ( 30 )
            /   \
          ( 15 )( 10 )
```

_Notice how every parent (e.g., 50, 40) is larger than its children._

The core operations of a heap are `insert` and `delete-min` (for a min-heap) or `delete-max` (for a max-heap). These operations are very fast, taking `O(log N)` time. Viewing the min/max element is even faster, at `O(1)`.

---

#### **Implementation using an Array**

While heaps are conceptually trees, they are almost always implemented using an **array** (or list) for simplicity and efficiency. The "complete" tree structure means there are no gaps, so we can map the tree nodes to array indices without wasting space.

**Concept:** The tree is stored level by level in the array.

**Block Diagram: Mapping Tree to Array**

```
      Tree            Array Representation (0-indexed)
      ( 10 )        Index: 0  1  2  3  4
      /    \        Value: [10,15,30,40,50]
    ( 15 )  ( 30 )
    /   \
  ( 40 )( 50 )
```

To navigate this "tree" within the array, we use simple mathematical formulas to find the parent and children of any node at index `i`:

- `parent(i) = floor((i - 1) / 2)`
    
- `left_child(i) = 2 * i + 1`
    
- `right_child(i) = 2 * i + 2`
    

Maintaining the Heap Property:

When we insert or delete elements, the heap property might be temporarily violated. We fix this with two key procedures:

- **Heapify-up (or Sift-up):** After an `insert`, the new element is added to the end of the array. It might be smaller than its parent (in a min-heap). We repeatedly swap it with its parent until it reaches its correct position. This "bubbles up" the new element.
    
- **Heapify-down (or Sift-down):** After a `delete-min`, the root is removed. We move the last element in the array to the root position to fill the gap. This new root is likely larger than its children. We repeatedly swap it with its smallest child until it "sinks down" to its correct position.
    

---

#### **As the underlying structure for a Priority Queue**

This is the most important application of a heap.

- What is a Priority Queue?
    
    A Priority Queue is an abstract data type—like a regular queue or stack—but with a twist. Every element has a "priority" associated with it. When you dequeue an element, you don't get the one that was first-in; you get the one with the highest priority.
    
- Heap as the Implementation:
    
    A Heap is the perfect data structure to implement a Priority Queue because its core properties map directly to what a Priority Queue needs.
    
    - If you use a **Min-Heap**, you get a min-priority queue where the element with the smallest value has the highest priority.
        
    - If you use a **Max-Heap**, you get a max-priority queue where the element with the largest value has the highest priority.
        

**Mapping Operations:**

|Priority Queue Operation|Corresponding Min-Heap Operation|Time Complexity|
|---|---|---|
|`insert(value)`|Add `value` to the array and perform **heapify-up**.|`O(log N)`|
|`extract_min()`|Remove the root, replace it with the last element, and perform **heapify-down**.|`O(log N)`|
|`get_min()`|Return the element at the root (index 0).|`O(1)`|

Because a heap can perform these core priority queue operations so efficiently, it is the standard choice for its implementation.

---
### **9. Graphs**

#### **What they are & Core Concepts (Vertex, Edge, etc.)**

A **Graph** is a non-linear data structure consisting of a collection of **vertices** (also called nodes) and **edges** that connect pairs of these vertices. It is one of the most flexible data structures, capable of modeling a vast range of real-world scenarios, such as social networks, road maps, and computer networks.

**Block Diagram & Key Terminology:**

```
      (A)-------(B)  <-- Undirected, Unweighted Edge
       | \       |
       |  \      |
       |   \     |
      (C)   (D)---->(E)  <-- Directed, Weighted Edge (Weight = 7)
       ^     |     /
       |     |    /
       +-----+---
         Cycle
```

- **Vertex (or Node):** The fundamental unit of a graph, represented by the circles (A, B, C, D, E).
    
- **Edge (or Link):** The connection between two vertices.
    
- **Undirected Graph:** Edges have no direction. A connection between A and B means you can go from A to B and from B to A.
    
- **Directed Graph (Digraph):** Edges have a direction, indicated by an arrow. An edge from D to E means you can go from D to E, but not necessarily from E to D.
    
- **Unweighted Graph:** Edges simply show a connection; they do not have a value or cost associated with them.
    
- **Weighted Graph:** Each edge has a numerical weight or cost associated with it, representing things like distance, time, or capacity (e.g., the edge D->E has a weight of 7).
    
- **Path:** A sequence of vertices connected by edges. (e.g., A -> B -> E is a path).
    
- **Cycle:** A path that starts and ends at the same vertex. (e.g., C -> D -> E -> C forms a cycle). A graph with no cycles is called **acyclic**.
    

---

#### **Representation: Adjacency List**

This is the most common way to represent a graph in practice, especially for graphs that are "sparse" (have relatively few edges).

- **Concept:** An adjacency list is an **array of lists** (or an array of linked lists, or a map of lists). The main array has one entry for every vertex in the graph. The list at index `i` contains all the vertices that are adjacent to vertex `i` (i.e., all vertices `j` for which an edge `(i, j)` exists).
    
- **For Weighted Graphs:** The list stores pairs of `(neighbor, weight)`.
    

**Block Diagram:**

**Graph:**

```
      (A) --1-- (B)
       | \
      4|  \2
       |   \
      (C)---3---(D)
```

**Adjacency List Representation:**

```
A: [ (B, 1), (C, 4), (D, 2) ]
B: [ (A, 1) ]
C: [ (A, 4), (D, 3) ]
D: [ (A, 2), (C, 3) ]
```

**Time & Space Complexity:**

- **Space Complexity:** `O(V + E)` where V is vertices and E is edges. We store one entry for each vertex and one entry for each edge.
    
- **Checking for an edge (A, B):** `O(k)` where `k` is the number of neighbors of A (degree of A). You have to scan A's list.
    
- **Iterating over all neighbors of A:** `O(k)` - Very efficient.
    

---

#### **Representation: Adjacency Matrix**

An adjacency matrix is a 2D array (a V x V matrix) that is better suited for "dense" graphs (graphs with many edges).

- **Concept:** The matrix `adjMatrix[i][j]` stores information about the edge between vertex `i` and vertex `j`.
    
    - **For Unweighted Graphs:** `adjMatrix[i][j] = 1` if an edge exists, and `0` otherwise.
        
    - **For Weighted Graphs:** `adjMatrix[i][j] = weight` if an edge exists, and `infinity` (or a special value like 0) otherwise.
        
- For an undirected graph, the matrix will be symmetric (`adjMatrix[i][j] == adjMatrix[j][i]`).
    

**Block Diagram:**

**Graph:**

```
      (A) --1-- (B)
       | \
      4|  \2
       |   \
      (C)---3---(D)
```

**Adjacency Matrix Representation (A=0, B=1, C=2, D=3):**

```
    A B C D
  +---------
A | 0 1 4 2
B | 1 0 0 0
C | 4 0 0 3
D | 2 0 3 0
```

**Time & Space Complexity:**

- **Space Complexity:** `O(V^2)`. This can be very large and wasteful for sparse graphs.
    
- **Checking for an edge (A, B):** `O(1)`. This is the main advantage. You can check for an edge in constant time by looking up `adjMatrix[A][B]`.
    
- **Iterating over all neighbors of A:** `O(V)`. You have to scan the entire row for vertex A to find its neighbors.
---
