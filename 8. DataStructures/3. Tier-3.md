### **10. Tries (Prefix Trees)**

#### **What it is & Core Concepts**

A **Trie** (pronounced "try," from the word re_trie_val), also known as a **Prefix Tree**, is a specialized tree-like data structure designed for storing and searching a dynamic set of strings. Unlike other trees that store keys in their nodes, a Trie's structure itself represents the keys.

Core Concept:

The position of a node in the tree defines the character it represents. Each path from the root to any node represents a prefix. A path from the root to a specially marked "end" node represents a complete word. This structure makes prefix-based operations, like autocomplete, incredibly fast.

Block Diagram:

Imagine inserting the words: "car", "cat", "cab", and "dog".

```
              (root)
              /    \
             /      \
           (c)      (d)
           /          \
         (a)          (o)
        / | \           \
      (b) (r) (t)         (g)
      (*) (*) (*)         (*)

(*) <-- This symbol indicates that the node is marked as an "End of Word".
```

- **Path `root -> c -> a`** represents the prefix `"ca"`.
    
- **Path `root -> c -> a -> t`** represents the complete word `"cat"` because the `(t)` node is marked as an end node.
    
- The word `"ca"` is a prefix but not a word in our set because the `(a)` node is not marked as an end node.
    

---

#### **Key Implementation (Node with children map and boolean flag)**

The implementation of a Trie relies on a specific node structure and three main operations: `insert`, `search`, and `startsWith`.

Node Structure

Each TrieNode needs two things:

1. **A collection of children:** This is typically a hash map (or dictionary) where keys are characters and values are pointers to other `TrieNode`s. An array of size 26 could also be used for lowercase English letters.
    
2. **An `isEndOfWord` flag:** A boolean that is `True` only if the node represents the end of a complete word that was inserted.
    

**Pseudocode for Node Structure:**

```
class TrieNode:
    children: map of {character -> TrieNode}
    isEndOfWord: boolean
```

Core Operations & Time Complexity

Let L be the length of the word or prefix being processed.

- **`insert(word)`**
    
    - **Concept:** Start at the root. For each character in the word, check if a path for that character exists in the current node's `children` map. If not, create a new `TrieNode` and add it. Then, move to that child node. After the last character, mark the final node's `isEndOfWord` flag as `True`.
        
    - **Time Complexity:** `O(L)` - You perform one step for each character in the word.
        
- **`search(word)`**
    
    - **Concept:** Start at the root. For each character in the word, check if a path exists. If at any point a path does not exist, the word is not in the Trie, so return `False`. If you successfully traverse the entire word, you must then check the final node's `isEndOfWord` flag. If it's `True`, the word exists. If it's `False`, the word is only a prefix of another word in the Trie, not a word itself.
        
    - **Time Complexity:** `O(L)`
        
- **`startsWith(prefix)`**
    
    - **Concept:** This is almost identical to `search`. You traverse the Trie based on the characters in the prefix. If the path exists for the entire prefix, you return `True`. You do **not** need to check the `isEndOfWord` flag.
        
    - **Time Complexity:** `O(L)`
        

**Pseudocode for `insert`:**

```
function insert(root, word):
    currentNode = root
    for each character in word:
        if character is not in currentNode.children:
            currentNode.children[character] = createNewTrieNode()
        currentNode = currentNode.children[character]
    currentNode.isEndOfWord = True
```
---
### **11. Union-Find / Disjoint Set Union (DSU)**

#### **What it is & Core Concepts**

A **Union-Find** (or **Disjoint Set Union**) data structure is a specialized data structure that tracks a partition of a set of elements into a number of non-overlapping (disjoint) subsets. It is incredibly efficient at determining if two elements are in the same subset and for merging two subsets together.

Core Concept & Analogy:

Imagine a set of people who are initially all strangers. Each person is in their own "group" of one. The DSU structure helps us perform two main actions:

1. **Union:** Two groups merge into one larger group (e.g., two friend circles merge).
    
2. **Find:** Quickly determine which group a specific person belongs to.
    

To do this, each group is represented as a tree, and the **root** of the tree serves as the unique **representative** for that entire group.

Implementation:

The structure is typically implemented with a simple array, often called parent.

- `parent[i]` stores the parent of element `i`.
    
- An element is the representative (root) of its group if it is its own parent, i.e., `parent[i] == i`.
    

Block Diagram: Initial State

For a set of 5 elements {0, 1, 2, 3, 4}, initially everyone is in their own set.

`parent` array: `[ 0, 1, 2, 3, 4 ]`

Visual Representation:

[0] [1] [2] [3] [4]

---

#### **Core Operations with Optimizations**

The power of the DSU comes from two core operations and their crucial optimizations.

**1. `find(i)` Operation**

- **Concept:** To find the representative of the set containing element `i`, you traverse up the tree by following the parent pointers until you reach a root (an element where `parent[i] == i`).
    
- **Optimization: Path Compression**
    
    - This is a critical optimization that dramatically speeds up the data structure.
        
    - **Logic:** After finding the root, on the way back down the recursion, you make every node on the traversal path point **directly** to the root. This flattens the tree structure.
        
    - **Benefit:** Future `find` operations for any of those nodes (or their children) will now take nearly constant time.
        

**Block Diagram: `find(4)` with Path Compression**

```
// Initial Tree Structure:
(0) -> (1) -> (2) -> (3) -> (4)  // A worst-case, list-like tree

// 1. find(4) traverses up: 4->3->2->1->0. It finds the root is 0.

// 2. Path Compression is applied:
//    parent[4] is set to 0.
//    parent[3] is set to 0.
//    parent[2] is set to 0.

// New Tree Structure:
      (0)
    / / \ \
  (1)(2)(3)(4)  // The tree is now flat and very efficient.
```

**2. `union(i, j)` Operation**

- **Concept:** To merge the sets containing elements `i` and `j`:
    
    1. Find the representative of `i` (let's call it `root_i`).
        
    2. Find the representative of `j` (let's call it `root_j`).
        
    3. If `root_i` is not the same as `root_j`, merge the two sets by setting the parent of one root to be the other (e.g., `parent[root_j] = root_i`).
        
- **Optimization: Union by Size or Rank**
    
    - This optimization prevents the trees from becoming tall and skinny (which would slow down the `find` operation).
        
    - **Logic (Union by Size):** Keep an extra array, `size`, where `size[i]` stores the number of elements in the set if `i` is a root. When performing a `union`, always attach the root of the **smaller** tree to the root of the **larger** tree.
        
    - **Benefit:** This ensures the trees stay as "bushy" and shallow as possible.
        

Time Complexity:

When both Path Compression and Union by Size/Rank are used, the time complexity of both find and union operations is amortized nearly constant time. It is technically written as O(α(N)), where α(N) is the Inverse Ackermann function. This function grows so slowly that for any practical input size N, its value is less than 5. For all interview purposes, it can be treated as O(1).

---
### **12. Segment Trees & Fenwick Trees (BIT)**

#### **What they are & Core Concepts for Range Queries**

Both **Segment Trees** and **Fenwick Trees (also known as Binary Indexed Trees or BITs)** are advanced, tree-like data structures built on top of an array. Their primary purpose is to answer **range queries** (e.g., "what is the sum/min/max of elements from index `i` to `j`?") and handle element **updates** far more efficiently than a simple array could.

The Problem They Solve:

Imagine you have an array of numbers.

- Finding the sum of a range `[i...j]` takes `O(N)` time.
    
- Updating an element at index `k` takes `O(1)` time.
    

If you have to perform many range sum queries, the `O(N)` cost becomes too slow. You could pre-calculate a prefix sum array, which makes range queries `O(1)`, but then updating an element becomes `O(N)` because you have to rebuild the entire prefix sum array.

Segment Trees and Fenwick Trees provide a compromise, allowing both operations to be completed in **`O(log N)` time**.

---

#### **Segment Tree Concepts & Lazy Propagation**

A **Segment Tree** is a very flexible binary tree that partitions an array into logarithmic segments.

- **Core Concept (Structure):**
    
    - The **root** of the tree represents the entire array (e.g., the sum of the range `[0...n-1]`).
        
    - Each internal node represents an interval, and its value is the result of merging its two children (e.g., `parent_sum = left_child_sum + right_child_sum`).
        
    - The **left child** represents the first half of the parent's interval, and the **right child** represents the second half.
        
    - The **leaf nodes** represent the individual elements of the original array.
        
- Block Diagram (for Range Sum):
    
    Array = [1, 5, 2, 8]
    
    ```
                  [0-3] Sum=16
                  /          \
          [0-1] Sum=6         [2-3] Sum=10
          /      \            /      \
     [0] Sum=1  [1] Sum=5  [2] Sum=2  [3] Sum=8
    ```
    
- **Range Query `O(log N)`:** To query a range, you traverse the tree. If a node's interval is completely contained within your query range, you use its pre-computed value and stop going down that path. This means you only need to traverse a few branches, leading to logarithmic time complexity.
    
- **Point Update `O(log N)`:** To update an element, you update the corresponding leaf node and then travel up the tree to the root, updating the value of all its ancestors along the path.
    
- **Lazy Propagation (for Range Updates):**
    
    - **Concept:** This is a crucial optimization for when you need to update an entire range (e.g., "add +2 to all elements from index `i` to `j`"). A naive update would be `O(N)`.
        
    - **Lazy Logic:** Instead of updating every leaf, you mark a high-level node with a "pending" or "lazy" update. This update is only pushed down to its children when a query or another update _needs_ to access those children. This allows range updates to also be done in `O(log N)` time.
        

---

#### **Fenwick Tree (BIT) Concepts & Bitwise Logic**

A **Fenwick Tree** is a more specialized and space-efficient alternative to a Segment Tree. It excels at **prefix sum** queries and point updates.

- **Core Concept (Structure):**
    
    - A BIT is an array (of the same size as the input array) where each index `i` is responsible for storing the sum of a specific, non-obvious range of the original array. It does not store a direct tree structure with nodes and pointers; the tree structure is **implicit** and is navigated using bitwise operations.
        
- **Bitwise Logic (The "Magic"):**
    
    - The key to a BIT is the operation `i & -i` (a bitwise AND between `i` and its two's complement). This operation isolates the **last set bit** (the rightmost '1' bit) of the number `i`.
        
    - **`update(index, value)`:** To update a value, you start at `index` in the BIT and add the new value. Then, you find the next index to update by calculating `index = index + (index & -i)`. You repeat this until you go past the array bounds. This traverses "up" the implicit tree.
        
    - **`query(index)` (for prefix sum):** To find the sum up to `index`, you start at `index` and add its value to your total. Then, you find the previous index to add by calculating `index = index - (index & -i)`. You repeat this until `index` becomes 0. This traverses "down" the implicit tree.
        
- Block Diagram (Conceptual Ranges):
    
    BIT Array
    
    ```
    Index 1: stores sum of original[0]
    Index 2: stores sum of original[0...1]
    Index 3: stores sum of original[2]
    Index 4: stores sum of original[0...3]
    Index 5: stores sum of original[4]
    Index 6: stores sum of original[4...5]
    Index 7: stores sum of original[6]
    Index 8: stores sum of original[0...7]
    ...
    ```
    
    This strange-looking responsibility assignment is what allows the bitwise logic to work correctly and efficiently.
    

**Comparison:**

- Use a **Segment Tree** when you need flexibility: range min/max/gcd queries, or complex range updates (e.g., "set range to value `x`").
    
- Use a **Fenwick Tree** when you specifically need **prefix sums** and point updates. It's faster to code and uses less memory.
---
### **13. Balanced Binary Search Trees**

#### **What they are & Core Concept of Self-Balancing**

A **Balanced Binary Search Tree** is a special type of Binary Search Tree (BST) that automatically keeps its height as small as possible. It enforces a "balance" condition to ensure that the tree remains "bushy" and does not become a long, skewed chain.

The Problem They Solve:

As we saw with regular BSTs, their performance is excellent (O(log N)) on average but degrades to a terrible O(N) in the worst case. This worst case happens when the tree becomes skewed, essentially turning into a linked list.

Block Diagram: The Problem - A Skewed BST

If you insert sorted numbers [10, 20, 30, 40, 50] into a standard BST, you get this:

```
      (10)
         \
         (20)
            \
            (30)
               \
               (40)
                  \
                  (50)  <-- Height is N-1. Search is O(N).
```

This is highly inefficient.

The Core Concept of Self-Balancing:

To prevent this, balanced BSTs perform small, local adjustments called rotations after every insert or delete operation. These rotations restructure the tree to maintain a balance condition, ensuring the height of the tree remains logarithmic with respect to the number of nodes (O(log N)).

Block Diagram: Tree Rotations

A rotation is a simple operation that changes the structure of the tree by swapping a parent and child node while preserving the BST ordering property.

_Example: A Left Rotation on node X_

```
     (Y)           (X)
    /   \         /   \
  (X)    c  <--> (a)   (Y)
 /   \                 /   \
(a)   b               b     c
```

This simple restructuring can reduce the height of a subtree and is the fundamental building block for all self-balancing algorithms.

---

#### **Conceptual Examples (AVL Trees, Red-Black Trees)**

You are almost never expected to implement these complex structures from scratch in an interview. However, you are expected to know what they are, why they exist, and what guarantees they provide.

**1. AVL Trees**

- **What it is:** The first self-balancing binary search tree to be invented.
    
- **Balance Condition:** For every single node in the tree, the **heights of its left and right subtrees cannot differ by more than 1**. This is a very strict balance condition.
    
- **How it works:** After an insertion or deletion, the tree checks the "balance factor" (height of left subtree - height of right subtree) of its ancestors. If any node's balance factor becomes -2 or +2, it performs one or two rotations to restore the balance.
    
- **Trade-offs:**
    
    - **Pro:** Because it's so strictly balanced, searches are extremely fast.
        
    - **Con:** The strict balancing requires frequent rotations, which can make insertions and deletions slightly slower compared to other balanced trees.
        

**Block Diagram: AVL Tree Insertion**

```
// 1. Initial balanced tree
      (20)
      /
    (10)

// 2. Insert 5. Tree is still balanced.
      (20)
      /
    (10)
    /
  (5)

// 3. Insert 3. The tree becomes unbalanced at node 20!
//    The height of 20's left subtree is 2, right is 0. Difference is 2.
      (20)
      /
    (10)
    /
  (5)
  /
(3)

// 4. A single rotation is performed at node 10 to fix the imbalance.
      (10)
      /  \
    (5)  (20)
    /
  (3)
```

**2. Red-Black Trees**

- **What it is:** Another type of self-balancing BST that uses a different, less strict balancing mechanism. It's the data structure used to implement `TreeMap` and `TreeSet` in Java, and `std::map` and `std::set` in C++.
    
- **Balance Condition:** It doesn't use height. Instead, it uses a set of rules based on coloring nodes either **Red** or **Black**:
    
    1. Every node is either Red or Black.
        
    2. The root is always Black.
        
    3. No two Red nodes can be adjacent (a Red node cannot have a Red parent or a Red child).
        
    4. Every path from a given node to any of its descendant `null` leaves contains the same number of Black nodes.
        
- **How it works:** These rules collectively ensure that the longest path in the tree is no more than twice as long as the shortest path, which keeps the tree approximately balanced and guarantees `O(log N)` height. Insertions and deletions may cause violations of these color rules, which are then fixed by a series of rotations and "re-colorings".
    
- **Trade-offs:**
    
    - **Pro:** It's less strictly balanced than an AVL tree, so it requires fewer rotations on average. This makes insertions and deletions faster.
        
    - **Con:** Searches can be slightly slower than in an AVL tree because the tree can be slightly less balanced.
        

**Conclusion:** Both AVL and Red-Black Trees solve the same problem: they are BSTs that **guarantee `O(log N)` time complexity for all major operations (search, insert, delete)** by automatically rebalancing themselves, thus avoiding the `O(N)` worst-case scenario of a standard BST.